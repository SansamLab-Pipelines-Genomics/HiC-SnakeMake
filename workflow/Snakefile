configfile: "config/config.yml"

##################################################################
##                    Define input functions                    ##
##################################################################

# this was created after reading 
#  https://eriqande.github.io/eca-bioinf-handbook/managing-workflows-with-snakemake.html
#  https://www.biostars.org/p/335903/

# this imports the pandas package functionality in an object named pd
import pandas as pd

# this reads the CSV file and sets an index using the values in the "sample" column.
samples_table = pd.read_csv("config/samples.csv").set_index("sample", drop=False)

# fastq filename input function definition set to Python dictionary
def fq_dict_from_sample(wildcards):
  return {
    "fq1": samples_table.loc[wildcards.sample, "fastq1"],
    "fq2": samples_table.loc[wildcards.sample, "fastq2"]
  }

##################################################################
##                           rules                              ##
##################################################################

# to run snakemake without explicitly requesting any output files on the command line, we must request output files in the first rule. Therefore we include this otherwise useless rule here  
rule all:
    input:
        expand("results/marked_pairs/{sample}.marked.sam.pairs.gz",sample=samples_table.index)
        #expand("results/cooler/{sample}."+str(config["cooler_bin_size"])+".cool",sample=samples_table.index)
        

rule run_bwa_mem:
#run-bwa-mem.sh
    # run-bwa-mem.sh <fastq1> <fastq2> <bwaIndex> <output_prefix> <nThreads>
            # fastq1, fastq2 : input fastq files, either gzipped or not
            # bwaIndex : tarball for bwa index, .tgz.
            # output_file : prefix of the output bam file.
            # nThreads : number of threads
    input:
        unpack(fq_dict_from_sample)
    params:
        bwaIndex=config["bwa_genome"],
        bwaThreads=config["bwa_threads"]
    output:
        bam="results/aligned/{sample}.bam"
    #conda:
    #    "envs/qc_trim_align.yml"
    log: "results/logs/snakelogs/run_bwa_mem.{sample}.log"
    shell:
        """
        bwa mem -t {params.bwaThreads} -SP5M {params.bwaIndex} {input.fq1} {input.fq2} | samtools view -Shb - > {output.bam}  
        """

rule make_pairs_with_pairtools_parse:
#run-pairsam-parse-sort.sh
# the following was copied from https://github.com/4dn-dcic/docker-4dn-hic/blob/master/scripts/run-pairsam-parse-sort.sh
# Classify Hi-C molecules as unmapped/single-sided/multimapped/chimeric/etc
    # and output one line per read, containing the following, separated by \\v:
    #  * triu-flipped pairs
    #  * read id
    #  * type of a Hi-C molecule
    #  * corresponding sam entries
    input:
        bam="results/aligned/{sample}.bam"
    params:
        nproc=config["pairtools_sort_nproc"],
        memory=config["pairtools_memory"],
        chrom_sizes=config["chrom_sizes"]
    output:
        pairs="results/pairs/{sample}_pairs.gz"
    log: "results/logs/snakelogs/sort_pairs_with_pairtools_sort.{sample}.log"
    shell:
        """
        pairtools parse -c {params.chrom_sizes} --drop-sam --add-columns mapq --output {output.pairs} {input.bam}
        """

rule sort_pairs_with_pairtools_sort:
    input:
        pairs="results/pairs/{sample}_pairs.gz"
    params:
        nproc=config["pairtools_sort_nproc"],
        memory=config["pairtools_memory"],
        tempdir="results/{sample}_temp/",
        chrom_sizes=config["chrom_sizes"]
    output:
        sorted_pairs="results/sorted_pairs/{sample}_sorted.pairs.gz"
    log: "results/logs/snakelogs/sort_pairs_with_pairtools_sort.{sample}.log"
    shell:
        """
        [ -d {params.tempdir} ] || mkdir {params.tempdir}
        pairtools sort --nproc {params.nproc} --memory {params.memory} --tmpdir {params.tempdir} --output {output.sorted_pairs} {input.pairs}
        rm -rf {params.tempdir}
        """

## Insert run-pairsam-merge.sh when needed
# https://github.com/4dn-dcic/docker-4dn-hic/blob/master/scripts/run-pairsam-merge.sh

rule mark_duplicates_with_pairtools_dedup:
#run-pairsam-markasdup.sh
    input:
        sorted_pairs="results/sorted_pairs/{sample}_sorted.pairs.gz"
    output:
        marked_pairs="results/marked_pairs/{sample}.marked.sam.pairs.gz"
    log: "results/logs/snakelogs/mark_duplicates_with_pairtools_dedup.{sample}.log"
    shell:
        """
        pairtools dedup --mark-dups --output-dups - --output-unmapped - --output {output.marked_pairs} {input.sorted_pairs}
        pairix {output.marked_pairs}
        """

rule filter_pairs:
#run-pairsam-filter.sh
    input:
        marked_pairs="results/marked_pairs/{sample}.marked.sam.pairs.gz"
    output:
        dedup_pairs="results/filtered_pairs/{sample}.dedup.pairs.gz",
        lossless_bam="results/filtered_pairs/{sample}.lossless.bam",
        unmapped_sam="results/filtered_pairs/{sample}.unmapped.sam.pairs.gz"
    params:
        temp_file="results/marked_pairs/{sample}_temp.gz",
        temp_file1="results/marked_pairs/{sample}_temp1.gz",
        chrom_sizes=config["chrom_sizes"]
    log: "results/logs/snakelogs/filter_pairs.{sample}.log"
    shell:
        """
        ## Generate lossless bam
        pairtools split --output-sam {output.lossless_bam} {input.marked_pairs}

        # Select UU, UR, RU reads
        pairtools select '(pair_type == "UU") or (pair_type == "UR") or (pair_type == "RU")' --output-rest {output.unmapped_sam} --output {params.temp_file} {input.marked_pairs}
        pairtools split --output-pairs {params.temp_file1} {params.temp_file}
        pairtools select 'True' --chrom-subset {params.chrom_sizes} -o {output.dedup_pairs} {params.temp_file1}
        pairix {output.dedup_pairs}  # sanity check & indexing
        """


rule add_frag2Pairs:
#run-addfrag2pairs.sh
    input:
        dedup_pairs="results/filtered_pairs/{sample}.dedup.pairs.gz"
    output:
        frag2_pairs="results/frag2_pairs/{sample}.frag2pairs.pairs.gz"
    params:
        restriction_file=config["juicer_RE_file"],
        frag2_pairs_basename="results/frag2_pairs/{sample}.frag2pairs.pairs"
    log: "results/logs/snakelogs/add_frag2Pairs.{sample}.log"
    shell:
        """
        gunzip -ck {input.dedup_pairs} | workflow/scripts/fragment_4dnpairs.pl -a - {params.frag2_pairs_basename} {params.restriction_file}
        bgzip -f {params.frag2_pairs_basename}
        pairix -f {output.frag2_pairs}
        """


rule run_cooler:
#run-cooler.sh
    input:
        frag2_pairs="results/frag2_pairs/{sample}.frag2pairs.pairs.gz"
    output:
        cooler="results/cooler/{sample}."+str(config["cooler_bin_size"])+".cool"
    params:
        chrom_sizes=config["chrom_sizes"],
        cooler_bin_size=config["cooler_bin_size"],
        cooler_n_cores=config["cooler_n_cores"],
        cooler_max_split=config["cooler_max_split"],
        cooler_tempchrsize="./{sample}_tempchrsize"
    log: "results/logs/snakelogs/run_cooler.{sample}.log"
    shell:
        """
        # use for all chromosomes and contigs
        cp {params.chrom_sizes} {params.cooler_tempchrsize}
            
        # make bin file (possibly skip?)
        # cooler makebins -o {params.cooler_tempchrsize}:{params.cooler_bin_size} {params.cooler_tempchrsize} {params.cooler_bin_size}

        # the cload command requires the chrom size file to exist besides the chrom size bin file.
        cooler cload pairix -p {params.cooler_n_cores} -s {params.cooler_max_split} {params.cooler_tempchrsize}:{params.cooler_bin_size} {input.frag2_pairs} {output.cooler}
        """

